{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from itertools import combinations\n",
    "from typing import List, Iterable, Tuple, Dict\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_mask_matrix(current_mask):\n",
    "    def create_indices(mask):\n",
    "        positive_el_number = 0\n",
    "        indices = []\n",
    "        for i, v in enumerate(mask):\n",
    "            if v == 1:\n",
    "                indices.append([i,positive_el_number])\n",
    "                positive_el_number += 1\n",
    "        return np.array(indices, dtype=np.int32)\n",
    "    \n",
    "    indices = tf.py_func(create_indices, [current_mask], tf.int32)\n",
    "    number_of_remaining_elements = tf.cast(tf.divide(tf.size(indices),2), tf.int32)\n",
    "    values = tf.py_func(lambda _indices: np.ones(len(_indices), dtype=np.int32),\n",
    "                        [indices], tf.int32)\n",
    "    shape = tf.stack([tf.size(current_mask),number_of_remaining_elements])\n",
    "    mask_matrix = tf.sparse_to_dense(sparse_indices=indices, sparse_values=values, output_shape=shape)\n",
    "    return mask_matrix\n",
    "\n",
    "\n",
    "def _test_get_sparse_mask_matrix(curr_mask, expected_mask_matrix):\n",
    "    mask = get_sparse_mask_matrix(curr_mask)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        result = session.run(mask)\n",
    "    assert np.array_equal(result, expected_mask_matrix)\n",
    "#     return num_remaining\n",
    "\n",
    "test_current_mask = tf.constant([1,0,1,0,1], dtype=tf.int32)\n",
    "expected_mask_matrix = np.array([[1,0,0],\n",
    "                                 [0,0,0],\n",
    "                                 [0,1,0],\n",
    "                                 [0,0,0],\n",
    "                                 [0,0,1]])\n",
    "\n",
    "_test_get_sparse_mask_matrix(test_current_mask, expected_mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_input_columns_with_current_mask(input_rows, current_mask):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix containing the input_rows with only the 'allowed' columns\n",
    "    \"\"\"\n",
    "    inputs_mask_matrix = get_sparse_mask_matrix(current_mask)\n",
    "    return tf.matmul(input_rows, inputs_mask_matrix)\n",
    "\n",
    "def _test_reduce_input_columns_with_current_mask(test_input_rows, test_current_mask, expected_reduced_inputs):\n",
    "    reduced_inputs = reduce_input_columns_with_current_mask(test_input_rows, test_current_mask)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        result = session.run(reduced_inputs)\n",
    "    assert np.array_equal(result, expected_reduced_inputs)\n",
    "#     return result\n",
    "\n",
    "test_inputs = tf.constant([[1,2,1,2,1],\n",
    "                           [1,2,0,2,1]])\n",
    "test_current_mask = tf.constant([1,0,1,0,1], dtype=tf.int32)\n",
    "expected_reduced_inputs = np.array([[1,1,1],\n",
    "                                    [1,0,1]])\n",
    "_test_reduce_input_columns_with_current_mask(test_inputs, test_current_mask, expected_reduced_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_group_subsets_frequent(prev_groups, group_idxs, group_size):\n",
    "    return all([(frozenset(sub_group) in prev_groups) for sub_group in combinations(group_idxs, group_size-1)])\n",
    "\n",
    "\n",
    "def get_mapped_groups_cube_idxs_and_new_els_count(groups_cube_idxs):\n",
    "    new_els = sorted(set([i for idx in groups_cube_idxs for i in idx]))\n",
    "    groups_cube_idxs_map = {idx: i for i, idx in enumerate(new_els)}\n",
    "    mapped_groups_cube_idxs = np.array([[groups_cube_idxs_map[i] for i in idx] for idx in groups_cube_idxs], \n",
    "                                       dtype=np.int32)\n",
    "    return mapped_groups_cube_idxs, len(new_els)\n",
    "\n",
    "\n",
    "def get_possible_groups_cube_and_mask(prev_group_indices: ndarray, group_size: int, current_mask):\n",
    "    \"\"\"\n",
    "    TODO: Fix for case when no group_size groups are possible\n",
    "    \"\"\"\n",
    "    prev_groups = set(map(frozenset, prev_group_indices))\n",
    "    possible_el_indices = set(prev_group_indices.flatten())\n",
    "    possible_el_combinations = list(combinations(possible_el_indices, group_size))\n",
    "    groups_cube_idxs = [group_idxs for group_idxs in possible_el_combinations \n",
    "                        if are_group_subsets_frequent(prev_groups, group_idxs, group_size)]\n",
    "    groups_cube_values = np.array([1 for _ in range(len(groups_cube_idxs))], dtype=np.int32)\n",
    "    new_possible_el_indices = set([i for idx in groups_cube_idxs for i in idx])\n",
    "    if (len(new_possible_el_indices) == 0):\n",
    "        raise ValueError('No possible elements')\n",
    "    mask = [1 if i in new_possible_el_indices else 0 for i in range(len(current_mask))]\n",
    "    mapped_groups_cube_idxs, new_tot_els = get_mapped_groups_cube_idxs_and_new_els_count(groups_cube_idxs)\n",
    "    groups_cube_shape = np.array([new_tot_els for _ in range(group_size)], dtype=np.int32)\n",
    "    return tf.sparse_to_dense(sparse_indices=mapped_groups_cube_idxs,\n",
    "                              output_shape=groups_cube_shape,\n",
    "                              sparse_values=groups_cube_values,), mask\n",
    "\n",
    "\n",
    "def _test_get_possible_groups_cube_and_mask(prev_group_indices, group_size, current_mask, \n",
    "                                            expected_groups_cube, expected_mask):\n",
    "    poss_groups, mask = get_possible_groups_cube_and_mask(prev_group_indices, group_size, current_mask)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        result = session.run(poss_groups)\n",
    "    assert np.array_equal(result, expected_groups_cube)\n",
    "    assert np.array_equal(mask, expected_mask)\n",
    "#     return result, mask\n",
    "    \n",
    "prev_group_indices_2d = np.array([[2,3],[2,4],[3,4], [3,5]])\n",
    "group_size = 3\n",
    "current_mask = np.array([0,0,1,1,1,1])\n",
    "expected_groups_cube = np.array([[[0,0,0],\n",
    "                                  [0,0,1],\n",
    "                                  [0,0,0]],\n",
    "                                 [[0,0,0],\n",
    "                                  [0,0,0],\n",
    "                                  [0,0,0]],\n",
    "                                 [[0,0,0],\n",
    "                                  [0,0,0],\n",
    "                                  [0,0,0]]])\n",
    "expected_mask = np.array([0,0,1,1,1,0])\n",
    "_test_get_possible_groups_cube_and_mask(prev_group_indices_2d, group_size, current_mask, \n",
    "                                        expected_groups_cube, expected_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_t_multiplied_by_transpose_permutations(input_groups, group_size):\n",
    "    # We have the masked vectorised input rows as a N+1 dimensional tensor\n",
    "    # Now multiply by N - 1 tensors whose axes have been permuted (on all but 0th axis) \n",
    "    cross_multiplied_input_groups = input_groups\n",
    "    dims_list = list(range(1, group_size + 1))\n",
    "    perm_list = list(range(0, group_size))\n",
    "    L = len(dims_list)\n",
    "    for dim in dims_list[:-1]: #permutations(range(1,current_N + 1)):\n",
    "        perm = [0] + [((i + dim + L - 1) % L) + 1 for i in dims_list]\n",
    "        print(f\"perm{perm}\")\n",
    "#         perm = [0 if i == -1 else perm[i] for i in range(-1, len(perm))]\n",
    "        input_groups_perm = tf.transpose(input_groups, perm=perm)\n",
    "        #TODO: Test broadcasting always works when using same number of input rows as unique elements\n",
    "        cross_multiplied_input_groups = tf.multiply(cross_multiplied_input_groups, input_groups_perm)\n",
    "    return cross_multiplied_input_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_next_mask_and_groups for group size 3\n",
      "perm[0, 2, 3, 1]\n",
      "perm[0, 3, 1, 2]\n",
      "Tensor(\"SparseToDense_136:0\", shape=(4, 4, 4), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[0, 1, 3],\n",
       "         [1, 2, 3]]), array([2, 2], dtype=int32), array([[0, 1, 3],\n",
       "         [1, 2, 3]])], [1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_mask_and_groups(gc):\n",
    "    \"\"\"\n",
    "    TODO: use tf.tensordot to join the input rows to the 'possible groups' cube\n",
    "    ----- The input elements axis becomes the new dimension of the \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Get_next_mask_and_groups for group size {gc['current_N']}\")\n",
    "    \n",
    "    inputs_reduced = reduce_input_columns_with_current_mask(gc[\"input_rows\"],\n",
    "                                                            gc[\"curr_bin_mask\"])\n",
    "    \n",
    "    possible_groups_cube, mask = get_possible_groups_cube_and_mask(gc[\"prev_group_indices\"], \n",
    "                                                                   gc[\"current_N\"], \n",
    "                                                                   gc[\"curr_bin_mask\"])\n",
    "    \n",
    "    shape = [gc[\"num_input_rows\"]] + [gc[\"num_remaining_els\"] for _ in range(0,gc[\"current_N\"])]\n",
    "    number_of_elements = gc[\"num_remaining_els\"]**(gc[\"current_N\"]-1)\n",
    "    expanded_inputs_t = tf.reshape(tf.tile(inputs_reduced, [1,number_of_elements]), shape)\n",
    "    \n",
    "    traspose_multiplied_input_groups = get_inputs_t_multiplied_by_transpose_permutations(expanded_inputs_t,\n",
    "                                                                                         gc[\"current_N\"])\n",
    "    print(possible_groups_cube)\n",
    "    input_groups_t = tf.multiply(traspose_multiplied_input_groups, possible_groups_cube)\n",
    "    \n",
    "    group_counts_t = tf.reduce_sum(input_groups_t, axis=0)\n",
    "\n",
    "    frequent_groups_indices = tf.where(group_counts_t > 0)\n",
    "    frequent_groups_counts = tf.gather_nd(group_counts_t, frequent_groups_indices)\n",
    "    next_possible_groups_indices = tf.where(group_counts_t >= gc[\"min_support\"])\n",
    "    \n",
    "    return frequent_groups_indices, frequent_groups_counts, next_possible_groups_indices, mask\n",
    "\n",
    "\n",
    "def test_get_next_mask_and_groups(gc):\n",
    "    with tf.Session() as session:\n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)\n",
    "        frequent_groups_indices, frequent_groups_counts, next_possible_groups_indices, next_mask = \\\n",
    "            get_next_mask_and_groups(gc)\n",
    "        group_counts_array = session.run([frequent_groups_indices,\n",
    "                                          frequent_groups_counts,\n",
    "                                          next_possible_groups_indices])\n",
    "    return group_counts_array, next_mask\n",
    "    \n",
    "original_input_els = ['A','B','C','D','E']\n",
    "vectorised_inputs_stack_2 = tf.Variable([[1,1,1,1,0],\n",
    "                                         [1,1,0,1,1],\n",
    "                                         [0,1,1,1,0]], tf.int32)\n",
    "frequent_bin_mask_2 = np.array([1,1,1,1,0], dtype=np.int32)\n",
    "prev_group_indices_2 = np.array([[0,1],[1,2],[2,3],[1,3],[0,3]], dtype=np.int32)\n",
    "\n",
    "input_3_5 = {\n",
    "    \"current_N\": 3,\n",
    "    \"prev_group_indices\": prev_group_indices_2,\n",
    "    \"original_els\": original_input_els, \n",
    "    \"num_original_els\": 5,\n",
    "    \"input_rows\": vectorised_inputs_stack_2,\n",
    "    \"num_input_rows\": 3,\n",
    "    \"curr_bin_mask\": frequent_bin_mask_2,\n",
    "    \"num_remaining_els\": 4,\n",
    "    \"curr_group_count_totals\": [], \n",
    "    \"min_support\": 1\n",
    "}\n",
    "\n",
    "test_get_next_mask_and_groups(input_3_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_collections_as_binary_arrays(input_collections: List[Iterable]) -> Tuple[List, List[ndarray]]:\n",
    "    \"\"\"\n",
    "    TODO: Get binarised_input_collections with tensorflow?\n",
    "    \"\"\"\n",
    "    # First get complete set of input elements\n",
    "    all_input_elements = list(sorted(set([el for row in input_collections for el in row])))\n",
    "    # For each input collection, create array with 1 for element exists or 0 for not exists \n",
    "    # (extension: count the number of elements - for use with multiple element counting version)\n",
    "    binarised_input_collections = [tf.Variable([1 if el in row else 0 for el in all_input_elements], dtype=tf.int32)\n",
    "                                   for row in input_collections]\n",
    "    return (all_input_elements, binarised_input_collections,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_next_mask_and_groups for group size 2\n",
      "perm[0, 2, 1]\n",
      "Tensor(\"SparseToDense_153:0\", shape=(4, 4), dtype=int32)\n",
      "[array([[0, 1],\n",
      "       [0, 2],\n",
      "       [0, 3],\n",
      "       [1, 2],\n",
      "       [1, 3]]), array([5, 2, 2, 2, 2], dtype=int32), array([[0, 1],\n",
      "       [0, 2],\n",
      "       [0, 3],\n",
      "       [1, 2],\n",
      "       [1, 3]])]\n",
      "Get_next_mask_and_groups for group size 3\n",
      "perm[0, 2, 3, 1]\n",
      "perm[0, 3, 1, 2]\n",
      "Tensor(\"SparseToDense_155:0\", shape=(4, 4, 4), dtype=int32)\n",
      "[array([[0, 1, 2],\n",
      "       [0, 1, 3]]), array([2, 2], dtype=int32), array([[0, 1, 2],\n",
      "       [0, 1, 3]])]\n",
      "Get_next_mask_and_groups for group size 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[0, 1, 2],\n",
       "         [0, 1, 3]]), array([2, 2], dtype=int32), array([[0, 1, 2],\n",
       "         [0, 1, 3]])], [1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GroupCounts = namedtuple('GroupCounts', 'group, count')\n",
    "\n",
    "class AprioriFrequentSets:\n",
    "    def __init__(self, group_counts_list: List[GroupCounts], min_supp):\n",
    "        self.group_counts_list = group_counts_list\n",
    "        self.min_support = min_supp\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Overrides the default implementation\"\"\"\n",
    "        if isinstance(self, other.__class__):\n",
    "            return self.__dict__ == other.__dict__\n",
    "        return False\n",
    "\n",
    "\n",
    "def apriori(input_id_to_collections, min_support:int=0) -> AprioriFrequentSets:\n",
    "    \"\"\"\n",
    "    TODO: handle case where input_rows is list type\n",
    "    TODO: when input_rows is dict with ids, also collect all the ids which are in the frequent item groups\n",
    "    \"\"\"\n",
    "    with tf.Session() as session:\n",
    "        all_input_elements, vectorised_input_collections = get_input_collections_as_binary_arrays(\n",
    "            input_id_to_collections.values()\n",
    "        )\n",
    "        original_els_count = len(all_input_elements)\n",
    "        original_input_rows_count = len(input_id_to_collections.keys())\n",
    "\n",
    "        number_of_unique_input_els = len(all_input_elements)\n",
    "\n",
    "        vectorised_inputs_stack = tf.stack(vectorised_input_collections)\n",
    "\n",
    "        single_el_occurances = tf.reduce_sum(vectorised_inputs_stack, axis=0)\n",
    "\n",
    "        # Get single set groups as itemsets (in original formats)\n",
    "        single_member_indices = tf.where(single_el_occurances >= 0)\n",
    "        single_member_groups = tf.gather_nd(all_input_elements, single_member_indices)\n",
    "        single_member_group_counts = tf.gather_nd(single_el_occurances, single_member_indices)\n",
    "        \n",
    "        next_el_indices = tf.where(single_el_occurances >= min_support)\n",
    "\n",
    "        frequent_single_bin_mask = tf.cast(single_el_occurances >= min_support, tf.int32)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)\n",
    "        \n",
    "        #Add these to the group counts total\n",
    "        single_mem_indices, single_mem_groups, single_mem_counts, single_el_mask, single_el_indices = \\\n",
    "            session.run([single_member_indices, \n",
    "                         single_member_groups, \n",
    "                         single_member_group_counts, \n",
    "                         frequent_single_bin_mask,\n",
    "                         next_el_indices])\n",
    "    \n",
    "        # TODO: add as frequent groups single_mem_indices\n",
    "\n",
    "        gc = {\n",
    "            \"current_N\": 1,\n",
    "            \"prev_group_indices\": single_el_indices,\n",
    "            \"original_els\": all_input_elements, \n",
    "            \"num_original_els\": original_els_count,\n",
    "            \"input_rows\": vectorised_input_collections,\n",
    "            \"num_input_rows\": original_input_rows_count,\n",
    "            \"curr_bin_mask\": single_el_mask,\n",
    "            \"num_remaining_els\": len(single_el_indices),\n",
    "            \"min_support\": min_support\n",
    "        }\n",
    "        for dim in range(2,5):\n",
    "            gc[\"current_N\"] = dim\n",
    "            \n",
    "            try:\n",
    "                frequent_groups_indices, frequent_groups_counts, next_possible_groups_indices, next_mask = \\\n",
    "                    get_next_mask_and_groups(gc)\n",
    "            except ValueError as err:\n",
    "                break\n",
    "                \n",
    "            group_counts_array = session.run([frequent_groups_indices,\n",
    "                                          frequent_groups_counts,\n",
    "                                          next_possible_groups_indices])\n",
    "            print(group_counts_array)\n",
    "            gc[\"prev_group_indices\"] = group_counts_array[2]\n",
    "            gc[\"curr_bin_mask\"] = next_mask\n",
    "            gc[\"num_remaining_els\"] = np.sum(next_mask)\n",
    "            #TODO: convert group_counts_array elements to GroupCounts objects and collect in some list\n",
    "        return group_counts_array, next_mask\n",
    "        \n",
    "def _test_apriori(input_rows: Dict, min_support, expected_apriori_frequent_sets: AprioriFrequentSets):\n",
    "    result = apriori(input_rows, min_support)\n",
    "    \n",
    "#     assert expected_apriori_frequent_sets == result\n",
    "    return result\n",
    "\n",
    "\n",
    "input_dict_1 = {0:['A','B','C'],\n",
    "                1:['A','B','C'],\n",
    "                2:['A','B','D'],\n",
    "                3:['A','B','D'],\n",
    "                4:['A','B','E']}\n",
    "min_support_1 = 2\n",
    "group_counts_list_1 = [GroupCounts({'A'},5),GroupCounts({'B'},5),GroupCounts({'C'},2),GroupCounts({'D'},2),\n",
    "                       GroupCounts({'E'},1),\n",
    "                       GroupCounts({'A','B'},5),GroupCounts({'A','C'},2),GroupCounts({'A','D'},2),\n",
    "                       GroupCounts({'B','C'},2),GroupCounts({'B','D'},2),\n",
    "                       GroupCounts({'A','B','C'},2),GroupCounts({'A','B','D'},2)]\n",
    "expected_apriori_frequent_sets_1 = AprioriFrequentSets(group_counts_list_1, min_support_1)\n",
    "\n",
    "_test_apriori(input_dict_1, min_support_1, expected_apriori_frequent_sets_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}