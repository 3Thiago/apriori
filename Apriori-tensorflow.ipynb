{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation of Apriori\n",
    "\n",
    "#### Matrix representation of apriori entities\n",
    "- Output: n-dimensional cube where nth dimension represents nth member of group. Zeros along diagonals.\n",
    "\n",
    "- Input: N by m dimensional matrix where N is the number of Cpc's and m is the number of samples (families).\n",
    "- Size-1 candidate sets: the 4-letter cpc codes\n",
    "- Size-1 frequency sets: adding input columns and filtering by whether greater than support threshold.\n",
    "\n",
    "\n",
    "\n",
    "- Size-n candidate sets: E.g. From the size-1 frequent sets, get all the possible pairings sets (removing reversed order duplicates)\n",
    "\n",
    "Taking 'mask' from n-1's > min_support filter - set zeros on all below threshold\n",
    "                       \n",
    "-------------------------------------Set main diagonal to zeros-------------\n",
    "\n",
    "---A---B---C---D-------------------------A---B---C---D-------------------------A---B---C---D\n",
    "   \n",
    "A--1---1---0---1----------------------A--0---1---0---1----------------------A--0---0---0---0\n",
    "\n",
    "B--1---1---0---1----------------------B--1---0---0---1----------------------B--1---0---0---0\n",
    "\n",
    "C--0---0---0---0----------------------C--0---0---0---0----------------------C--0---0---0---0\n",
    "\n",
    "D--1---1---0---1----------------------D--1---1---0---0----------------------D--1---1---0---0\n",
    "\n",
    "\n",
    "- Make mask for each input collection, extending into the N'th dimension, then stack all input collections together before count op.\n",
    "\n",
    "#### Stages of completion\n",
    "1. Get something to count single occurances of elements in input collections\n",
    "2. Create and run against test data -> optimise, e.g. use sparse tensor representations \n",
    "3. Return richer output e.g. support, confidence, lift measures for each group (wrapped in some container)\n",
    "4. Store the result in memory for further requests e.g. distance measures between groups etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from itertools import permutations, combinations\n",
    "import collections\n",
    "\n",
    "from typing import Dict, Any, List, Set, Iterable, Tuple\n",
    "\n",
    "GroupCountIO = collections.namedtuple('GroupCountIO', \n",
    "                                      \"\"\"\n",
    "                                      current_N,\n",
    "                                      original_els, num_original_els, \n",
    "                                      input_rows, num_input_rows,\n",
    "                                      curr_bin_mask, curr_group_count_totals,\n",
    "                                      min_support\n",
    "                                      \"\"\")\n",
    "GroupSupport = collections.namedtuple('GroupSupport', 'groups, support, group_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_collections_as_binary_arrays(input_collections: List[Iterable]) -> Tuple[List, List[array]]:\n",
    "    \"\"\"\n",
    "    TODO: Get binarised_input_collections with tensorflow?\n",
    "    \"\"\"\n",
    "    # First get complete set of input elements\n",
    "    all_input_elements = list(sorted(set([el for row in input_collections for el in row])))\n",
    "    # For each input collection, create array with 1 for element exists or 0 for not exists \n",
    "    # (extension: count the number of elements - for use with multiple element counting version)\n",
    "    binarised_input_collections = [tf.Variable([1 if el in row else 0 for el in all_input_elements], dtype=tf.uint8)\n",
    "                                   for row in input_collections]\n",
    "    return (all_input_elements, binarised_input_collections,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_groups perm: [0, 1]\n",
      "get_groups perm: [1, 0]\n",
      "(array([[b'A', b'B'],\n",
      "       [b'A', b'D'],\n",
      "       [b'B', b'D'],\n",
      "       [b'C', b'D']], dtype=object), array([1, 1, 1, 1], dtype=int32), 2)\n",
      "get_groups perm: [0, 1, 2]\n",
      "get_groups perm: [1, 2, 0]\n",
      "get_groups perm: [2, 0, 1]\n",
      "(array([[b'A', b'B', b'C'],\n",
      "       [b'A', b'B', b'D'],\n",
      "       [b'B', b'C', b'D']], dtype=object), array([1, 1, 5], dtype=int32), 3)\n"
     ]
    }
   ],
   "source": [
    "def get_groups(original_input_els, inputs_size, group_summations,  group_summations_dim) -> Dict[frozenset, int]:\n",
    "    \"\"\"\n",
    "    TODO: Write down an expected format for the group_summations shape\n",
    "    TODO: If this takes lots of memory, just pass the group_summations out of session and recursively search\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs_size: The number of distinct input elements\n",
    "    original_input_els: The original distinct input elements which have the same order as the axes in group_summations\n",
    "                        and which hence can be mapped back by index.\n",
    "    group_summations: The N dimensional tensor containing the groups which met the min support threshold.\n",
    "                      Each dimension is of length len(original_input_els)\n",
    "    inputs_size: The length of unique elements in the input\n",
    "    group_summations_dim: The number of dimensions in group_summations\n",
    "    returns: The dictionary of group elements to group support\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: group_summations_dim can probably be replaced with -1 or tf.newaxis\n",
    "    \n",
    "    if group_summations_dim <= 1:\n",
    "        raise ValueError(\"group_summations_dim should have greater than one dimension\")\n",
    "        \n",
    "    dim_to_append = original_input_els\n",
    "    for dim in range(1,group_summations_dim):\n",
    "        dim_rows = []\n",
    "        for row in range(inputs_size):\n",
    "            dim_rows.append(dim_to_append)\n",
    "        group_dims_stack = tf.stack(dim_rows)\n",
    "        dim_to_append = group_dims_stack\n",
    "\n",
    "    total_possible_groups_list = []\n",
    "    dims_list = list(range(0, group_summations_dim))\n",
    "    for dim in dims_list:\n",
    "        perm = [(i + dim) % len(dims_list) for i in dims_list]\n",
    "        print(f\"get_groups perm: {perm}\")\n",
    "        group_dims_stack_perm = tf.transpose(group_dims_stack, perm=perm)\n",
    "        dim_possible_groups_perm = tf.expand_dims(group_dims_stack_perm, group_summations_dim)\n",
    "        total_possible_groups_list.append(dim_possible_groups_perm)\n",
    "    group_layout_tensor = tf.concat(total_possible_groups_list, group_summations_dim)\n",
    "\n",
    "    successfull_groups = tf.gather_nd(group_layout_tensor, tf.where(group_summations >= 1))\n",
    "    successfull_group_counts = tf.gather_nd(group_summations, tf.where(group_summations >= 1))\n",
    "    return successfull_groups, successfull_group_counts, tf.constant(group_summations_dim)\n",
    "\n",
    "\n",
    "test_original_input_els = ['A','B','C','D']\n",
    "\n",
    "test_group_summations_2 = tf.Variable([[0,0,0,0], \n",
    "                                       [1,0,0,0], \n",
    "                                       [0,0,0,0], \n",
    "                                       [1,1,1,0]], tf.uint8)\n",
    "expected_output_2 = tuple([[[b'A',b'B'],[b'A',b'D'],[b'B',b'D'],[b'C',b'D']], [1,1,1,1], 2])\n",
    "\n",
    "test_group_summations_3 = tf.Variable([\n",
    "    [[0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0]], \n",
    "    \n",
    "    [[0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0]],\n",
    "    \n",
    "    [[0,0,0,0], \n",
    "     [1,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0]],\n",
    "    \n",
    "    [[0,0,0,0], \n",
    "     [1,0,0,0], \n",
    "     [0,5,0,0], \n",
    "     [0,0,0,0]]\n",
    "], tf.uint8)\n",
    "expected_output_3 = tuple([np.array([[b'A',b'B',b'C'],[b'A',b'B',b'D'],[b'B',b'C',b'D']]),np.array([1,1,5]), 3])\n",
    "\n",
    "def test_get_groups(test_original_input_els, len_input_els, test_group_summations, \n",
    "                    len_group_summations_dims, expected):\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        result = session.run(get_groups(test_original_input_els, len_input_els,\n",
    "                                        test_group_summations, len_group_summations_dims))\n",
    "        print(result)\n",
    "    for i_group in range(len(result[0])):\n",
    "        assert len(result[0][i_group]) == len(expected[0][i_group])\n",
    "        for i_group_el in range(len(result[0][i_group])):\n",
    "            assert result[0][i_group][i_group_el] == expected[0][i_group][i_group_el]\n",
    "\n",
    "test_get_groups(test_original_input_els, 4, test_group_summations_2, 2, expected_output_2)\n",
    "test_get_groups(test_original_input_els, 4, test_group_summations_3, 3, expected_output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_with_mask(is_frequent_mask, input_row):\n",
    "#     return [1 if is_frequent_mask[i] else 0 for i in range(0, tf.size(input_row))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from resources.test_resources import expected_occluded_output_3d, input_occluded_output_3d\n",
    "\n",
    "def get_inputs_filtered_by_possible_combinations(group_dims_stack,\n",
    "                                                 current_els_mask, \n",
    "                                                 group_size):\n",
    "    \"\"\"\n",
    "    group_dims_stack: the \n",
    "    current_els_mask: python binary list with allowed elements as 1.\n",
    "    \"\"\"\n",
    "    # possible_els_idxs -> [0,3,5,10] = oth, 3rd etc els allowed.\n",
    "    possible_combinations_idxs = tf.py_func(lambda mask: \n",
    "                                   np.array(list(combinations([i for i, v in enumerate(mask) if v == 1], group_size)), \n",
    "                                            dtype=np.int32),\n",
    "                                   [current_els_mask], tf.int32)\n",
    "    \n",
    "    number_of_unique_input_els = tf.size(current_els_mask)\n",
    "    \n",
    "    mask_shape = tf.map_fn(lambda x: number_of_unique_input_els, tf.Variable(list(range(group_size))))\n",
    "        \n",
    "    values = tf.py_func(lambda perms: np.ones(len(perms), dtype=np.int32),\n",
    "                                                [possible_combinations_idxs], tf.int32) \n",
    "\n",
    "    allowed_combs = tf.sparse_to_dense(sparse_indices=possible_combinations_idxs, \n",
    "                                       output_shape=mask_shape,\n",
    "                                       sparse_values=values,)\n",
    "\n",
    "    return tf.multiply(allowed_combs, group_dims_stack)\n",
    " \n",
    "    \n",
    "def _test_get_diagonal_and_upper_zeroed_tensor(expected, group_dims_stack, current_els_mask, group_size):\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        tensor = get_inputs_filtered_by_possible_combinations(group_dims_stack, current_els_mask, group_size)\n",
    "        result = session.run(tensor)\n",
    "#     print('Expected:')\n",
    "#     print(str(np.array(expected)))\n",
    "#     print('Output:')\n",
    "#     print(str(result))\n",
    "    assert np.array_equal(np.array(expected), result)\n",
    "\n",
    "current_N = None\n",
    "_test_get_diagonal_and_upper_zeroed_tensor(expected_occluded_output_3d, input_occluded_output_3d, [1,1,1,1,1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_tensor_multiplied_by_transpose_permutations(group_dims_stack, group_size):\n",
    "    # We have the masked vectorised input rows as a N+1 dimensional tensor\n",
    "    # Now multiply by N - 1 tensors whose axes have been permuted (on all but 0th axis) \n",
    "    cross_multiplied_group_dims_stack = group_dims_stack\n",
    "    dims_list = list(range(1, group_size + 1))\n",
    "    perm_list = list(range(0, group_size))\n",
    "    L = len(dims_list)\n",
    "    for dim in dims_list[:-1]: #permutations(range(1,current_N + 1)):\n",
    "        perm = [0] + [((i + dim + L - 1) % L) + 1 for i in dims_list]\n",
    "#         perm = [0 if i == -1 else perm[i] for i in range(-1, len(perm))]\n",
    "        print(\"perm: \" + str(perm))\n",
    "        group_dims_stack_perm = tf.transpose(group_dims_stack, perm=perm)\n",
    "        #TODO: Test broadcasting always works when using same number of input rows as unique elements\n",
    "        cross_multiplied_group_dims_stack = tf.multiply(cross_multiplied_group_dims_stack, group_dims_stack_perm)\n",
    "    return cross_multiplied_group_dims_stack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_count_tensor_reduced_to_1D(group_count_t, number_of_unique_input_els, group_size):\n",
    "    \"\"\"\n",
    "    TODO: remove this and create get_next_mask_and_groups.input_collections_reduced with previous \n",
    "    next_frequent_bin_filter i.e. would be better to create next possible from combinations of current groups\n",
    "    e.g. if A,B,C and B,C,D are groups, but no size 3 group contains A and D, A,B,C,D should not be a possiblity\n",
    "    \"\"\"\n",
    "    ONE = tf.constant(1, dtype=tf.uint8)\n",
    "    ZERO = tf.constant(0, dtype=tf.uint8)\n",
    "    output_mask = tf.zeros(number_of_unique_input_els, dtype=tf.uint8)\n",
    "    dims_list = list(range(0, group_size))\n",
    "    dim_skips_list = [[d for d in dims_list if d != skip_d] for skip_d in dims_list]\n",
    "    print(f\"dim_skips_list: {dim_skips_list}\")\n",
    "    for dims in dim_skips_list:\n",
    "        print(dims)\n",
    "        output_mask = tf.add(output_mask, tf.reduce_sum(group_count_t, dims))\n",
    "        \n",
    "    return tf.unstack(tf.map_fn(lambda x: tf.cond(x >= ONE, true_fn=lambda: ONE, false_fn=lambda: ZERO), \n",
    "                     output_mask, dtype=tf.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_next_mask_and_groups for group size 3\n",
      "5\n",
      "perm: [0, 2, 1]\n",
      "perm: [0, 2, 3, 1]\n",
      "perm: [0, 3, 1, 2]\n",
      "get_groups perm: [0, 1, 2]\n",
      "get_groups perm: [1, 2, 0]\n",
      "get_groups perm: [2, 0, 1]\n",
      "dim_skips_list: [[1, 2], [0, 2], [0, 1]]\n",
      "[1, 2]\n",
      "[0, 2]\n",
      "[0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 1, 1],\n",
       " GroupSupport(groups=array([[b'C', b'B', b'A'],\n",
       "       [b'D', b'B', b'A'],\n",
       "       [b'E', b'B', b'A'],\n",
       "       [b'D', b'C', b'A'],\n",
       "       [b'E', b'C', b'A'],\n",
       "       [b'E', b'D', b'A'],\n",
       "       [b'D', b'C', b'B'],\n",
       "       [b'E', b'C', b'B'],\n",
       "       [b'E', b'D', b'B'],\n",
       "       [b'E', b'D', b'C']], dtype=object), support=array([2, 2, 1, 2, 1, 1, 3, 1, 1, 1], dtype=int32), group_size=3))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_mask_and_groups(gc: GroupCountIO) -> GroupSupport:\n",
    "    \"\"\"\n",
    "    Produces the next \n",
    "    \n",
    "    frequent_bin_mask: \n",
    "    The 1 dimensional tensor procuded by filtering elements in the previous iteration by min_support and reducing\n",
    "    back to single dimension.\n",
    "    It has 1s and 0s at each element denoting the presence of each group.\n",
    "    \n",
    "    current_N:\n",
    "    The dimension that is currently being created i.e. the new mask will be this large and the full count \n",
    "    representation tensor will be order N+1\n",
    "    \n",
    "    current_N,\n",
    "    original_els, num_original_els, \n",
    "    input_rows, num_input_rows,\n",
    "    curr_bin_mask, curr_group_count_totals,\n",
    "    min_support\n",
    "    \"\"\"\n",
    "    # TODO: return if number of frequent els < current_N\n",
    "\n",
    "    # TODO: find a better way to combine the N-dimensional hypercube mask with the #input-rows by N input \n",
    "    # vectorisation to get the group count representation\n",
    "    # E.g. tf.matmul or tf.tensordot\n",
    "    # Downside of the current (tile, reshape) form is that we throw away the count rep tensor\n",
    "    \n",
    "    print(f\"Get_next_mask_and_groups for group size {gc.current_N}\")\n",
    "    \n",
    "    input_collections_reduced = tf.multiply(gc.input_rows, gc.curr_bin_mask)\n",
    "    print(gc.num_original_els)\n",
    "    dim_to_append = input_collections_reduced\n",
    "    for d in range(2, gc.current_N+1):\n",
    "        group_dims_stack = tf.stack([dim_to_append for _ in range(0, gc.num_original_els)], axis=-1)\n",
    "        traspose_multiplied_stack = get_inputs_tensor_multiplied_by_transpose_permutations(group_dims_stack, d)\n",
    "        dim_to_append = traspose_multiplied_stack\n",
    "        \n",
    "    filtered_counts_tensor = get_inputs_filtered_by_possible_combinations(traspose_multiplied_stack,\n",
    "                                                                          gc.curr_bin_mask,\n",
    "                                                                          gc.current_N\n",
    "                                                                         )\n",
    "    \n",
    "    next_el_occurances = tf.reduce_sum(filtered_counts_tensor, axis=0)\n",
    "    \n",
    "    group_counts = get_groups(gc.original_els, gc.num_original_els, next_el_occurances, gc.current_N)\n",
    "    group_support = GroupSupport(*group_counts)\n",
    "\n",
    "    next_frequent_bin_filter = tf.cast(next_el_occurances >= gc.min_support, tf.uint8)\n",
    "    next_frequent_bin_mask = get_group_count_tensor_reduced_to_1D(next_frequent_bin_filter, gc.num_original_els,\n",
    "                                                                  gc.current_N)\n",
    "\n",
    "    return next_frequent_bin_mask, group_support\n",
    "    \n",
    "\n",
    "def test_get_next_mask_and_groups(gc: GroupCountIO):\n",
    "    with tf.Session() as session:\n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)\n",
    "        result = session.run(get_next_mask_and_groups(gc))\n",
    "    return result\n",
    "    \n",
    "#     current_N\n",
    "#     original_els, num_original_els, \n",
    "#     input_rows, num_input_rows,\n",
    "#     curr_bin_mask, curr_group_count_totals,\n",
    "#     min_support\n",
    "    \n",
    "original_input_els = ['A','B','C','D','E']\n",
    "vectorised_inputs_stack_2 = tf.Variable([[1,1,1,1,0],\n",
    "                                         [1,1,1,1,1], \n",
    "                                         [0,1,1,1,0]], tf.uint8)\n",
    "frequent_bin_mask_2 = tf.Variable([1,1,1,1,1], tf.uint8)\n",
    "\n",
    "input_3_5 = GroupCountIO(3, original_input_els, 5, vectorised_inputs_stack_2, 3, frequent_bin_mask_2, [], 1)\n",
    "results = test_get_next_mask_and_groups(input_3_5)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_groups(gc: GroupCountIO):\n",
    "    \"\"\"\n",
    "    GroupCountIO\n",
    "      current_N\n",
    "      original_els, num_original_els, \n",
    "      input_rows, num_input_rows,\n",
    "      curr_bin_mask, curr_group_count_totals,\n",
    "      min_support    \n",
    "    \"\"\"\n",
    "    next_mask, group_counts = get_next_mask_and_groups(gc)\n",
    "    gc.curr_group_count_totals.append(group_counts)\n",
    "    current_N += 1\n",
    "    return gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apriori_groups(input_id_to_collections: Dict, min_support: int) -> Set[frozenset]:\n",
    "    input_collections_size = len(input_id_to_collections.items())\n",
    "    \n",
    "    all_input_elements, vectorised_input_collections = get_input_collections_as_binary_arrays(\n",
    "        input_id_to_collections.values()\n",
    "    )\n",
    "\n",
    "    number_of_unique_input_els = len(all_input_elements)\n",
    "    \n",
    "    vectorised_inputs_stack = tf.stack(vectorised_input_collections)\n",
    "\n",
    "    single_el_occurances = tf.reduce_sum(vectorised_inputs_stack, axis=0)\n",
    "    \n",
    "    # Get single set groups as itemsets (in original formats)\n",
    "    single_member_groups = tf.gather_nd(all_input_elements, tf.where(single_el_occurances >= min_support))\n",
    "    single_member_group_counts = tf.gather_nd(single_el_occurances, tf.where(single_el_occurances >= min_support))\n",
    "    \n",
    "    frequent_single_bin_mask = tf.cast(single_el_occurances >= min_support, tf.uint8)\n",
    "        \n",
    "#     current_N\n",
    "#     original_els, num_original_els, \n",
    "#     input_rows, num_input_rows,\n",
    "#     curr_bin_mask, curr_group_count_totals,\n",
    "#     min_support\n",
    "\n",
    "    input_1 = (GroupCountIO(2, \n",
    "                            all_input_elements, number_of_unique_input_els,\n",
    "                            vectorised_input_collections, input_collections_size,\n",
    "                            frequent_single_bin_mask, \n",
    "                            [GroupSupport(single_member_groups, single_member_group_counts, 1)],\n",
    "                            min_support\n",
    "                           ),)\n",
    "    \n",
    "    condition = lambda group_count_io: tf.greater(tf.reduce_sum(group_count_io.curr_bin_mask), \n",
    "                                                  tf.constant(0, dtype=tf.uint8))\n",
    "    \n",
    "    group_count_output = tf.while_loop(condition, get_next_groups, input_1,\n",
    "                                       parallel_iterations=1,\n",
    "                                       back_prop=False,\n",
    "                                       maximum_iterations=len(original_input_els))\n",
    "    \n",
    "    return group_count_output.curr_group_count_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_next_mask_and_groups for group size Tensor(\"while_29/Identity_1:0\", shape=(), dtype=int32)\n",
      "Tensor(\"while_29/Identity_9:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-ce6a84bd5626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                              frozenset(['B','C','D','E'])])\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_apriori_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_groups_min_sup_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-221-ce6a84bd5626>\u001b[0m in \u001b[0;36mtest_apriori_groups\u001b[0;34m(input_groups, real_groups, min_support)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnumber_of_unique_input_els\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_apriori_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_groups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_apriori_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-48c87dcf73b1>\u001b[0m in \u001b[0;36mget_apriori_groups\u001b[0;34m(input_id_to_collections, min_support)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                        \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                        \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                        maximum_iterations=len(original_input_els))\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgroup_count_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_group_count_totals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[1;32m   2932\u001b[0m         swap_memory=swap_memory)\n\u001b[1;32m   2933\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2935\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2718\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2720\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2721\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2660\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2661\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2663\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2906\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2907\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(lv)))\n\u001b[0;32m-> 2908\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-edcf21f8d528>\u001b[0m in \u001b[0;36mget_next_groups\u001b[0;34m(gc)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mmin_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnext_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_mask_and_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_group_count_totals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcurrent_N\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-b7e4d0a14907>\u001b[0m in \u001b[0;36mget_next_mask_and_groups\u001b[0;34m(gc)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_els\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdim_to_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_collections_reduced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_N\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mgroup_dims_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_to_append\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_els\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         traspose_multiplied_stack = get_inputs_tensor_multiplied_by_transpose_permutations(group_dims_stack,\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "current_N = None\n",
    "number_of_unique_input_els = None\n",
    "def test_apriori_groups(input_groups: Dict[Any,str], real_groups, min_support=2):\n",
    "    result_groups = get_apriori_groups(input_groups, min_support)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        result = session.run(result_groups)\n",
    "        \n",
    "    print(result)\n",
    "#     assert len([g for g in real_groups if g in result_groups]) == len(real_groups)\n",
    "    \n",
    "groups = {'0':['A','B','C','D','E'], '1':['B','C','D','E','F'],'2':['C','D','E','F','G']}\n",
    "\n",
    "real_groups_min_sup_2 = set([frozenset(['B']),frozenset(['C']),frozenset(['D']),frozenset(['E']),frozenset(['F']),\n",
    "                             frozenset(['B','C']),frozenset(['C','D']),frozenset(['D','E']),frozenset(['B','D']),\n",
    "                             frozenset(['B','E']),frozenset(['C','E']),\n",
    "                             frozenset(['B','C','D']),frozenset(['B','C','E']),frozenset(['C','D','E']),frozenset(['C','D','F']),\n",
    "                             frozenset(['C','E','F']),frozenset(['D','E','F']),\n",
    "                             frozenset(['B','C','D','E'])])\n",
    "\n",
    "test_apriori_groups(groups, real_groups_min_sup_2, min_support=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-c9d434821599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "[1, 3, 4] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"SparseTensor_1/indices:0\", shape=(2, 2), dtype=int64), values=Tensor(\"SparseTensor_1/values:0\", shape=(2,), dtype=int32), dense_shape=Tensor(\"SparseTensor_1/dense_shape:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x110db62e8>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2e3971d53a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#     result = session.run(tf.shape(smt))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   2796\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 2798\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   2799\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m               raise TypeError(\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    231\u001b[0m                                          as_ref=False):\n\u001b[1;32m    232\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    210\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    211\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 212\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    213\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/miniconda3/envs/apriori/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    500\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    501\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"SparseTensor_1/indices:0\", shape=(2, 2), dtype=int64), values=Tensor(\"SparseTensor_1/values:0\", shape=(2,), dtype=int32), dense_shape=Tensor(\"SparseTensor_1/dense_shape:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "smt = tf.Variable([[1,0,0],\n",
    "                   [1,0,1],\n",
    "                   [1,0,1]])\n",
    "smt_2 = tf.Variable(\n",
    "    [\n",
    "        [[1,1,1],\n",
    "         [1,1,1],\n",
    "         [1,1,1]],\n",
    "        [[1,1,1],\n",
    "         [1,1,1],\n",
    "         [1,1,1]],\n",
    "        [[1,1,1],\n",
    "         [1,1,1],\n",
    "         [1,1,1]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "sparse = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    "\n",
    "sparse_2 = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(tf.multiply(sparse_2,sparse))\n",
    "#     result = session.run(tf.shape(smt))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1 2 1]\n",
      " [1 2 0 2 1]]\n",
      "*\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 1]]\n",
      "=\n",
      "[[1 0 1 0 1]\n",
      " [1 0 0 0 1]]\n",
      "\n",
      "[[1 2 1 2 1]\n",
      " [1 2 0 2 1]]\n",
      "*\n",
      "[[0.5 0. ]\n",
      " [0.  0. ]\n",
      " [0.5 0.5]\n",
      " [0.  0. ]\n",
      " [0.5 0.5]]\n",
      "=\n",
      "[[1.5 1. ]\n",
      " [1.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vs = np.matrix('1 2 1 2 1; 1 2 0 2 1')\n",
    "M = np.matrix(\n",
    "    '1 0 0 0 0;'   # col 1 becomes col 1\n",
    "    '0 0 0 0 0;'   # drop col 2\n",
    "    '0 0 1 0 0;'   # col 3 becomes col 2\n",
    "    '0 0 0 0 0;'   # drop col 4\n",
    "    '0 0 0 0 1',   # col 5 becomes col 3\n",
    ")\n",
    "print(vs)\n",
    "print('*')\n",
    "print(M)\n",
    "print('=')\n",
    "print(vs*M)\n",
    "\n",
    "print()\n",
    "M2 = np.matrix(\n",
    "    '0.5 0   ;'  # col 1 becomes 0.5 of col 1, and 0.5 of col 3\n",
    "    '0   0   ;'  # col 2 dropped\n",
    "    '0.5 0.5 ;'  # col 3 becomes 0.5 of col 1, and 0.5 of col 2\n",
    "    '0   0   ;'  # col 4 droppes\n",
    "    '0.5 0.5 ')  # col 5 becomes 0.5 of col 2\n",
    "\n",
    "print(vs)\n",
    "print('*')\n",
    "print(M2)\n",
    "print('=')\n",
    "print(vs*M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "invalid data type for einsum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-01698e17b09a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mM_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: invalid data type for einsum"
     ]
    }
   ],
   "source": [
    "vs = np.array([['1','2','1','2','1'], '1','2','0','2','1'])\n",
    "M = np.array(\n",
    "    [\n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']]\n",
    "    ]\n",
    ")\n",
    "type(M)\n",
    "M_t = np.transpose(M,[1,2,0])\n",
    "np.matmul(M_t, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, Pair(j=32, k=64))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "Pair = collections.namedtuple('Pair', 'j, k')\n",
    "ijk_0 = (tf.constant(0), Pair(tf.constant(1), tf.constant(2)))\n",
    "c = lambda i, p: i < 10\n",
    "b = lambda i, p: (i + 1, Pair((p.j + p.k), (p.j - p.k)))\n",
    "ijk_final = tf.while_loop(c, b, ijk_0)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(ijk_final)\n",
    "#     result = session.run(tf.shape(smt))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
