{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation of Apriori\n",
    "\n",
    "#### Matrix representation of apriori entities\n",
    "- Output: n-dimensional cube where nth dimension represents nth member of group. Zeros along diagonals.\n",
    "\n",
    "- Input: N by m dimensional matrix where N is the number of Cpc's and m is the number of samples (families).\n",
    "- Size-1 candidate sets: the 4-letter cpc codes\n",
    "- Size-1 frequency sets: adding input columns and filtering by whether greater than support threshold.\n",
    "\n",
    "\n",
    "\n",
    "- Size-n candidate sets: E.g. From the size-1 frequent sets, get all the possible pairings sets (removing reversed order duplicates)\n",
    "\n",
    "Taking 'mask' from n-1's > min_support filter - set zeros on all below threshold\n",
    "                       \n",
    "-------------------------------------Set main diagonal to zeros-------------\n",
    "\n",
    "---A---B---C---D-------------------------A---B---C---D-------------------------A---B---C---D\n",
    "   \n",
    "A--1---1---0---1----------------------A--0---1---0---1----------------------A--0---0---0---0\n",
    "\n",
    "B--1---1---0---1----------------------B--1---0---0---1----------------------B--1---0---0---0\n",
    "\n",
    "C--0---0---0---0----------------------C--0---0---0---0----------------------C--0---0---0---0\n",
    "\n",
    "D--1---1---0---1----------------------D--1---1---0---0----------------------D--1---1---0---0\n",
    "\n",
    "\n",
    "- Make mask for each input collection, extending into the N'th dimension, then stack all input collections together before count op.\n",
    "\n",
    "#### Stages of completion\n",
    "1. Get something to count single occurances of elements in input collections\n",
    "2. Get benefits of apriori - reduce tensor size as inputs are ruled-out\n",
    "3. Create and run against test data -> optimise, e.g. use sparse tensor representations \n",
    "4. Return richer output e.g. support, confidence, lift measures for each group (wrapped in some container)\n",
    "5. Store the result in memory for further requests e.g. distance measures between groups etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from itertools import permutations, combinations\n",
    "import collections\n",
    "\n",
    "from typing import Dict, Any, List, Set, Iterable, Tuple\n",
    "\n",
    "GroupCounts = collections.namedtuple('GroupCounts', 'group_counts, size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_collections_as_binary_arrays(input_collections: List[Iterable]) -> Tuple[List, List[array]]:\n",
    "    \"\"\"\n",
    "    TODO: Get binarised_input_collections with tensorflow?\n",
    "    \"\"\"\n",
    "    # First get complete set of input elements\n",
    "    all_input_elements = list(sorted(set([el for row in input_collections for el in row])))\n",
    "    # For each input collection, create array with 1 for element exists or 0 for not exists \n",
    "    # (extension: count the number of elements - for use with multiple element counting version)\n",
    "    binarised_input_collections = [tf.Variable([1 if el in row else 0 for el in all_input_elements], dtype=tf.int32)\n",
    "                                   for row in input_collections]\n",
    "    return (all_input_elements, binarised_input_collections,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[b'A', b'B'],\n",
      "       [b'A', b'D'],\n",
      "       [b'B', b'D'],\n",
      "       [b'C', b'D']], dtype=object), array([1, 1, 1, 1], dtype=int32), 2)\n",
      "(array([[b'A', b'B', b'C'],\n",
      "       [b'A', b'B', b'D'],\n",
      "       [b'B', b'C', b'D']], dtype=object), array([1, 1, 5], dtype=int32), 3)\n"
     ]
    }
   ],
   "source": [
    "def get_groups(original_input_els, inputs_size, group_summations,  group_summations_dim) -> Dict[frozenset, int]:\n",
    "    \"\"\"\n",
    "    TODO: Write down an expected format for the group_summations shape\n",
    "    TODO: If this takes lots of memory, just pass the group_summations out of session and recursively search\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs_size: The number of distinct input elements\n",
    "    original_input_els: The original distinct input elements which have the same order as the axes in group_summations\n",
    "                        and which hence can be mapped back by index.\n",
    "    group_summations: The N dimensional tensor containing the groups which met the min support threshold.\n",
    "                      Each dimension is of length len(original_input_els)\n",
    "    inputs_size: The length of unique elements in the input\n",
    "    group_summations_dim: The number of dimensions in group_summations\n",
    "    returns: The dictionary of group elements to group support\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: group_summations_dim can probably be replaced with -1 or tf.newaxis\n",
    "    \n",
    "    if group_summations_dim <= 1:\n",
    "        raise ValueError(\"group_summations_dim should have greater than one dimension\")\n",
    "        \n",
    "    dim_to_append = original_input_els\n",
    "    for dim in range(1,group_summations_dim):\n",
    "        dim_rows = []\n",
    "        for row in range(inputs_size):\n",
    "            dim_rows.append(dim_to_append)\n",
    "        group_dims_stack = tf.stack(dim_rows)\n",
    "        dim_to_append = group_dims_stack\n",
    "\n",
    "    total_possible_groups_list = []\n",
    "    dims_list = list(range(0, group_summations_dim))\n",
    "    for dim in dims_list:\n",
    "        perm = [(i + dim) % len(dims_list) for i in dims_list]\n",
    "        group_dims_stack_perm = tf.transpose(group_dims_stack, perm=perm)\n",
    "        dim_possible_groups_perm = tf.expand_dims(group_dims_stack_perm, group_summations_dim)\n",
    "        total_possible_groups_list.append(dim_possible_groups_perm)\n",
    "    group_layout_tensor = tf.concat(total_possible_groups_list, group_summations_dim)\n",
    "\n",
    "    successfull_groups = tf.gather_nd(group_layout_tensor, tf.where(group_summations >= 1))\n",
    "    successfull_group_counts = tf.gather_nd(group_summations, tf.where(group_summations >= 1))\n",
    "    return successfull_groups, successfull_group_counts, tf.constant(group_summations_dim)\n",
    "\n",
    "\n",
    "test_original_input_els = ['A','B','C','D']\n",
    "\n",
    "test_group_summations_2 = tf.Variable([[0,0,0,0], \n",
    "                                       [1,0,0,0], \n",
    "                                       [0,0,0,0], \n",
    "                                       [1,1,1,0]], tf.int32)\n",
    "expected_output_2 = tuple([[[b'A',b'B'],[b'A',b'D'],[b'B',b'D'],[b'C',b'D']], [1,1,1,1], 2])\n",
    "\n",
    "test_group_summations_3 = tf.Variable([\n",
    "    [[0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0]], \n",
    "    \n",
    "    [[0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0]],\n",
    "    \n",
    "    [[0,0,0,0], \n",
    "     [1,0,0,0], \n",
    "     [0,0,0,0], \n",
    "     [0,0,0,0]],\n",
    "    \n",
    "    [[0,0,0,0], \n",
    "     [1,0,0,0], \n",
    "     [0,5,0,0], \n",
    "     [0,0,0,0]]\n",
    "], tf.int32)\n",
    "expected_output_3 = tuple([np.array([[b'A',b'B',b'C'],[b'A',b'B',b'D'],[b'B',b'C',b'D']]),np.array([1,1,5]), 3])\n",
    "\n",
    "def test_get_groups(test_original_input_els, len_input_els, test_group_summations, \n",
    "                    len_group_summations_dims, expected):\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        result = session.run(get_groups(test_original_input_els, len_input_els,\n",
    "                                        test_group_summations, len_group_summations_dims))\n",
    "        print(result)\n",
    "    for i_group in range(len(result[0])):\n",
    "        assert len(result[0][i_group]) == len(expected[0][i_group])\n",
    "        for i_group_el in range(len(result[0][i_group])):\n",
    "            assert result[0][i_group][i_group_el] == expected[0][i_group][i_group_el]\n",
    "\n",
    "test_get_groups(test_original_input_els, 4, test_group_summations_2, 2, expected_output_2)\n",
    "test_get_groups(test_original_input_els, 4, test_group_summations_3, 3, expected_output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_with_mask(is_frequent_mask, input_row):\n",
    "#     return [1 if is_frequent_mask[i] else 0 for i in range(0, tf.size(input_row))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from resources.test_resources import expected_occluded_output_3d, input_occluded_output_3d\n",
    "\n",
    "def get_inputs_filtered_by_possible_combinations(group_dims_stack,\n",
    "                                                 current_els_mask, \n",
    "                                                 group_size):\n",
    "    \"\"\"\n",
    "    group_dims_stack: the \n",
    "    current_els_mask: python binary list with allowed elements as 1.\n",
    "    \"\"\"\n",
    "    # possible_els_idxs -> [0,3,5,10] = oth, 3rd etc els allowed.\n",
    "    possible_combinations_idxs = tf.py_func(lambda mask: \n",
    "                                   np.array(list(combinations([i for i, v in enumerate(mask) if v == 1], group_size)), \n",
    "                                            dtype=np.int32),\n",
    "                                   [current_els_mask], tf.int32)\n",
    "    \n",
    "    number_of_unique_input_els = tf.size(current_els_mask)\n",
    "    \n",
    "    mask_shape = tf.map_fn(lambda x: number_of_unique_input_els, tf.Variable(list(range(group_size))))\n",
    "        \n",
    "    values = tf.py_func(lambda perms: np.ones(len(perms), dtype=np.int32),\n",
    "                                                [possible_combinations_idxs], tf.int32) \n",
    "\n",
    "    allowed_combs = tf.sparse_to_dense(sparse_indices=possible_combinations_idxs, \n",
    "                                       output_shape=mask_shape,\n",
    "                                       sparse_values=values,)\n",
    "\n",
    "    return tf.multiply(allowed_combs, group_dims_stack)\n",
    " \n",
    "    \n",
    "def _test_get_diagonal_and_upper_zeroed_tensor(expected, group_dims_stack, current_els_mask, group_size):\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        tensor = get_inputs_filtered_by_possible_combinations(group_dims_stack, current_els_mask, group_size)\n",
    "        result = session.run(tensor)\n",
    "#     print('Expected:')\n",
    "#     print(str(np.array(expected)))\n",
    "#     print('Output:')\n",
    "#     print(str(result))\n",
    "    assert np.array_equal(np.array(expected), result)\n",
    "\n",
    "current_N = None\n",
    "_test_get_diagonal_and_upper_zeroed_tensor(expected_occluded_output_3d, input_occluded_output_3d, [1,1,1,1,1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_tensor_multiplied_by_transpose_permutations(group_dims_stack, group_size):\n",
    "    # We have the masked vectorised input rows as a N+1 dimensional tensor\n",
    "    # Now multiply by N - 1 tensors whose axes have been permuted (on all but 0th axis) \n",
    "    cross_multiplied_group_dims_stack = group_dims_stack\n",
    "    dims_list = list(range(1, group_size + 1))\n",
    "    perm_list = list(range(0, group_size))\n",
    "    L = len(dims_list)\n",
    "    for dim in dims_list[:-1]: #permutations(range(1,current_N + 1)):\n",
    "        perm = [0] + [((i + dim + L - 1) % L) + 1 for i in dims_list]\n",
    "#         perm = [0 if i == -1 else perm[i] for i in range(-1, len(perm))]\n",
    "        group_dims_stack_perm = tf.transpose(group_dims_stack, perm=perm)\n",
    "        #TODO: Test broadcasting always works when using same number of input rows as unique elements\n",
    "        cross_multiplied_group_dims_stack = tf.multiply(cross_multiplied_group_dims_stack, group_dims_stack_perm)\n",
    "    return cross_multiplied_group_dims_stack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_count_tensor_reduced_to_1D(group_count_t, number_of_unique_input_els, group_size):\n",
    "    \"\"\"\n",
    "    TODO: remove this and create get_next_mask_and_groups.input_collections_reduced with previous \n",
    "    next_frequent_bin_filter i.e. would be better to create next possible from combinations of current groups\n",
    "    e.g. if A,B,C and B,C,D are groups, but no size 3 group contains A and D, A,B,C,D should not be a possiblity\n",
    "    \"\"\"\n",
    "    ONE = tf.constant(1, dtype=tf.int32)\n",
    "    ZERO = tf.constant(0, dtype=tf.int32)\n",
    "    output_mask = tf.zeros(number_of_unique_input_els, dtype=tf.int32)\n",
    "    dims_list = list(range(0, group_size))\n",
    "    dim_skips_list = [[d for d in dims_list if d != skip_d] for skip_d in dims_list]\n",
    "    for dims in dim_skips_list:\n",
    "        output_mask = tf.add(output_mask, tf.reduce_sum(group_count_t, dims))\n",
    "        \n",
    "    return tf.unstack(tf.map_fn(lambda x: tf.cond(x >= ONE, true_fn=lambda: ONE, false_fn=lambda: ZERO), \n",
    "                     output_mask, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_next_mask_and_groups for group size 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 1, 1],\n",
       " {'N': 3,\n",
       "  'counts': array([2, 2, 1, 2, 1, 1, 3, 1, 1, 1], dtype=int32),\n",
       "  'groups': array([[b'C', b'B', b'A'],\n",
       "         [b'D', b'B', b'A'],\n",
       "         [b'E', b'B', b'A'],\n",
       "         [b'D', b'C', b'A'],\n",
       "         [b'E', b'C', b'A'],\n",
       "         [b'E', b'D', b'A'],\n",
       "         [b'D', b'C', b'B'],\n",
       "         [b'E', b'C', b'B'],\n",
       "         [b'E', b'D', b'B'],\n",
       "         [b'E', b'D', b'C']], dtype=object)})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_mask_and_groups(gc):\n",
    "    \"\"\"\n",
    "    Produces the next \n",
    "    \n",
    "    frequent_bin_mask: \n",
    "    The 1 dimensional tensor procuded by filtering elements in the previous iteration by min_support and reducing\n",
    "    back to single dimension.\n",
    "    It has 1s and 0s at each element denoting the presence of each group.\n",
    "    \n",
    "    current_N:\n",
    "    The dimension that is currently being created i.e. the new mask will be this large and the full count \n",
    "    representation tensor will be order N+1\n",
    "    \n",
    "    current_N,\n",
    "    original_els, num_original_els, \n",
    "    input_rows, num_input_rows,\n",
    "    curr_bin_mask, curr_group_count_totals,\n",
    "    min_support\n",
    "    \"\"\"\n",
    "    # TODO: return if number of frequent els < current_N\n",
    "\n",
    "    # TODO: find a better way to combine the N-dimensional hypercube mask with the #input-rows by N input \n",
    "    # vectorisation to get the group count representation\n",
    "    # E.g. tf.matmul or tf.tensordot\n",
    "    # Downside of the current (tile, reshape) form is that we throw away the count rep tensor\n",
    "    \n",
    "    print(f\"Get_next_mask_and_groups for group size {gc['current_N']}\")\n",
    "    \n",
    "    input_collections_reduced = tf.multiply(gc[\"input_rows\"], gc[\"curr_bin_mask\"])\n",
    "    dim_to_append = input_collections_reduced\n",
    "    for d in range(2, gc[\"current_N\"]+1):\n",
    "        group_dims_stack = tf.stack([dim_to_append for _ in range(0, gc[\"num_original_els\"])], axis=-1)\n",
    "        traspose_multiplied_stack = get_inputs_tensor_multiplied_by_transpose_permutations(group_dims_stack, d)\n",
    "        dim_to_append = traspose_multiplied_stack\n",
    "        \n",
    "    filtered_counts_tensor = get_inputs_filtered_by_possible_combinations(traspose_multiplied_stack,\n",
    "                                                                          gc[\"curr_bin_mask\"],\n",
    "                                                                          gc[\"current_N\"]\n",
    "                                                                         )\n",
    "    \n",
    "    next_el_occurances = tf.reduce_sum(filtered_counts_tensor, axis=0)\n",
    "    \n",
    "    groups, counts, N = get_groups(gc[\"original_els\"], gc[\"num_original_els\"], next_el_occurances, gc[\"current_N\"])\n",
    "    group_support = {\n",
    "        \"groups\": groups,\n",
    "        \"counts\": counts,\n",
    "        \"N\": N\n",
    "    }\n",
    "\n",
    "    next_frequent_bin_filter = tf.cast(next_el_occurances >= gc[\"min_support\"], tf.int32)\n",
    "    next_frequent_bin_mask = get_group_count_tensor_reduced_to_1D(next_frequent_bin_filter, gc[\"num_original_els\"],\n",
    "                                                                  gc[\"current_N\"])\n",
    "\n",
    "    return next_frequent_bin_mask, group_support\n",
    "    \n",
    "\n",
    "def test_get_next_mask_and_groups(gc):\n",
    "    with tf.Session() as session:\n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)\n",
    "        result = session.run(get_next_mask_and_groups(gc))\n",
    "    return result\n",
    "    \n",
    "#     current_N\n",
    "#     original_els, num_original_els, \n",
    "#     input_rows, num_input_rows,\n",
    "#     curr_bin_mask, curr_group_count_totals,\n",
    "#     min_support\n",
    "    \n",
    "original_input_els = ['A','B','C','D','E']\n",
    "vectorised_inputs_stack_2 = tf.Variable([[1,1,1,1,0],\n",
    "                                         [1,1,1,1,1], \n",
    "                                         [0,1,1,1,0]], tf.int32)\n",
    "frequent_bin_mask_2 = tf.Variable([1,1,1,1,1], tf.int32)\n",
    "\n",
    "input_3_5 = {\n",
    "        \"current_N\": 3,\n",
    "        \"original_els\": original_input_els, \n",
    "        \"num_original_els\": 5,\n",
    "        \"input_rows\": vectorised_inputs_stack_2,\n",
    "        \"num_input_rows\": 3,\n",
    "        \"curr_bin_mask\": frequent_bin_mask_2,\n",
    "        \"curr_group_count_totals\": [], \n",
    "        \"min_support\": 1\n",
    "    }\n",
    "results = test_get_next_mask_and_groups(input_3_5)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_grouped(session, groups_t, counts_t, size_t) -> GroupCounts:\n",
    "    group_result = session.run(groups_t)\n",
    "    group_support_result = session.run(counts_t)\n",
    "    group_counts = list(zip(group_result, group_support_result))\n",
    "    size = session.run(tf.constant([size_t], dtype=tf.int32))\n",
    "    return GroupCounts(group_counts, size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apriori_groups(input_id_to_collections: Dict, min_support: int):\n",
    "    input_collections_size = len(input_id_to_collections.items())\n",
    "    \n",
    "    all_input_elements, vectorised_input_collections = get_input_collections_as_binary_arrays(\n",
    "        input_id_to_collections.values()\n",
    "    )\n",
    "\n",
    "    number_of_unique_input_els = len(all_input_elements)\n",
    "    \n",
    "    vectorised_inputs_stack = tf.stack(vectorised_input_collections)\n",
    "\n",
    "    single_el_occurances = tf.reduce_sum(vectorised_inputs_stack, axis=0)\n",
    "    \n",
    "    # Get single set groups as itemsets (in original formats)\n",
    "    single_member_groups = tf.gather_nd(all_input_elements, tf.where(single_el_occurances >= min_support))\n",
    "    single_member_group_counts = tf.gather_nd(single_el_occurances, tf.where(single_el_occurances >= min_support))\n",
    "    \n",
    "    frequent_single_bin_mask = tf.cast(single_el_occurances >= min_support, tf.int32)\n",
    "        \n",
    "#     current_N\n",
    "#     original_els, num_original_els, \n",
    "#     input_rows, num_input_rows,\n",
    "#     curr_bin_mask, curr_group_count_totals,\n",
    "#     min_support\n",
    "\n",
    "    group_one_support = {\n",
    "        \"groups\": single_member_groups,\n",
    "        \"counts\": single_member_group_counts,\n",
    "        \"N\": 1\n",
    "    }\n",
    "    next_input = {\n",
    "        \"current_N\": 2,\n",
    "        \"original_els\": all_input_elements, \n",
    "        \"num_original_els\": number_of_unique_input_els,\n",
    "        \"input_rows\": vectorised_input_collections,\n",
    "        \"num_input_rows\": input_collections_size,\n",
    "        \"curr_bin_mask\": frequent_single_bin_mask,\n",
    "        \"curr_group_count_totals\": None, \n",
    "        \"min_support\": min_support\n",
    "    }   \n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        final_results = []\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        session.run(init)\n",
    "        \n",
    "#         group_one_support_result = session.run(group_one_support)\n",
    "#         single_member_group_counts_result = session.run(single_member_group_counts)\n",
    "#         single_member_group_counts = list(zip(group_one_support_result, single_member_group_counts_result))\n",
    "        \n",
    "        final_results.append(get_results_grouped(session, \n",
    "                                                 single_member_groups, \n",
    "                                                 single_member_group_counts, \n",
    "                                                 1))\n",
    "        #TODO: Fix checking min_support before getting groups\n",
    "        \n",
    "        for d in range(4): #range(number_of_unique_input_els): \n",
    "            # Should be much fewer, will break loop when no groups > min_supp\n",
    "            \n",
    "            next_mask, group_counts = get_next_mask_and_groups(next_input)\n",
    "            next_input[\"curr_bin_mask\"] = next_mask\n",
    "            next_input[\"current_N\"] = next_input[\"current_N\"] + 1\n",
    "            \n",
    "            final_results.append(\n",
    "                get_results_grouped(session,\n",
    "                                    group_counts['groups'], \n",
    "                                    group_counts['counts'], \n",
    "                                    d)\n",
    "            )\n",
    "            \n",
    "            total_group_support = session.run(tf.reduce_sum(next_mask))\n",
    "            print(f\"Total support = {total_group_support}\")\n",
    "            if total_group_support == 0:\n",
    "                break\n",
    "            \n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get_next_mask_and_groups for group size 2\n",
      "Total support = 5\n",
      "Get_next_mask_and_groups for group size 3\n",
      "Total support = 5\n",
      "Get_next_mask_and_groups for group size 4\n",
      "Total support = 5\n",
      "Get_next_mask_and_groups for group size 5\n",
      "Total support = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[GroupCounts(group_counts=[(b'B', 2), (b'C', 3), (b'D', 3), (b'E', 3), (b'F', 2)], size=1),\n",
       " GroupCounts(group_counts=[(array([b'C', b'B'], dtype=object), 2), (array([b'D', b'B'], dtype=object), 2), (array([b'E', b'B'], dtype=object), 2), (array([b'F', b'B'], dtype=object), 1), (array([b'D', b'C'], dtype=object), 3), (array([b'E', b'C'], dtype=object), 3), (array([b'F', b'C'], dtype=object), 2), (array([b'E', b'D'], dtype=object), 3), (array([b'F', b'D'], dtype=object), 2), (array([b'F', b'E'], dtype=object), 2)], size=0),\n",
       " GroupCounts(group_counts=[(array([b'D', b'C', b'B'], dtype=object), 2), (array([b'E', b'C', b'B'], dtype=object), 2), (array([b'F', b'C', b'B'], dtype=object), 1), (array([b'E', b'D', b'B'], dtype=object), 2), (array([b'F', b'D', b'B'], dtype=object), 1), (array([b'F', b'E', b'B'], dtype=object), 1), (array([b'E', b'D', b'C'], dtype=object), 3), (array([b'F', b'D', b'C'], dtype=object), 2), (array([b'F', b'E', b'C'], dtype=object), 2), (array([b'F', b'E', b'D'], dtype=object), 2)], size=1),\n",
       " GroupCounts(group_counts=[(array([b'E', b'D', b'C', b'B'], dtype=object), 2), (array([b'F', b'D', b'C', b'B'], dtype=object), 1), (array([b'F', b'E', b'C', b'B'], dtype=object), 1), (array([b'F', b'E', b'D', b'B'], dtype=object), 1), (array([b'F', b'E', b'D', b'C'], dtype=object), 2)], size=2),\n",
       " GroupCounts(group_counts=[(array([b'F', b'E', b'D', b'C', b'B'], dtype=object), 1)], size=3)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_apriori_groups(input_groups: Dict[Any,str], real_groups, min_support=2):\n",
    "    result_groups = get_apriori_groups(input_groups, min_support)\n",
    "\n",
    "    return result_groups\n",
    "#     assert len([g for g in real_groups if g in result_groups]) == len(real_groups)\n",
    "    \n",
    "groups = {'0':['A','B','C','D','E'], '1':['B','C','D','E','F'],'2':['C','D','E','F','G']}\n",
    "\n",
    "real_groups_min_sup_2 = set([frozenset(['B']),frozenset(['C']),frozenset(['D']),frozenset(['E']),frozenset(['F']),\n",
    "                             frozenset(['B','C']),frozenset(['C','D']),frozenset(['D','E']),frozenset(['B','D']),\n",
    "                             frozenset(['B','E']),frozenset(['C','E']),\n",
    "                             frozenset(['B','C','D']),frozenset(['B','C','E']),frozenset(['C','D','E']),frozenset(['C','D','F']),\n",
    "                             frozenset(['C','E','F']),frozenset(['D','E','F']),\n",
    "                             frozenset(['B','C','D','E'])])\n",
    "\n",
    "result = test_apriori_groups(groups, real_groups_min_sup_2, min_support=2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smt = tf.Variable([[1,0,0],\n",
    "                   [1,0,1],\n",
    "                   [1,0,1]])\n",
    "smt_2 = tf.Variable(\n",
    "    [\n",
    "        [[1,1,1],\n",
    "         [1,1,1],\n",
    "         [1,1,1]],\n",
    "        [[1,1,1],\n",
    "         [1,1,1],\n",
    "         [1,1,1]],\n",
    "        [[1,1,1],\n",
    "         [1,1,1],\n",
    "         [1,1,1]]\n",
    "    ]\n",
    ")\n",
    "\n",
    "sparse = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    "\n",
    "sparse_2 = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(tf.multiply(sparse_2,sparse))\n",
    "#     result = session.run(tf.shape(smt))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1 2 1]\n",
      " [1 2 0 2 1]]\n",
      "*\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 1]]\n",
      "=\n",
      "[[1 0 1 0 1]\n",
      " [1 0 0 0 1]]\n",
      "\n",
      "[[1 2 1 2 1]\n",
      " [1 2 0 2 1]]\n",
      "*\n",
      "[[0.5 0. ]\n",
      " [0.  0. ]\n",
      " [0.5 0.5]\n",
      " [0.  0. ]\n",
      " [0.5 0.5]]\n",
      "=\n",
      "[[1.5 1. ]\n",
      " [1.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vs = np.matrix('1 2 1 2 1; 1 2 0 2 1')\n",
    "M = np.matrix(\n",
    "    '1 0 0 0 0;'   # col 1 becomes col 1\n",
    "    '0 0 0 0 0;'   # drop col 2\n",
    "    '0 0 1 0 0;'   # col 3 becomes col 2\n",
    "    '0 0 0 0 0;'   # drop col 4\n",
    "    '0 0 0 0 1',   # col 5 becomes col 3\n",
    ")\n",
    "print(vs)\n",
    "print('*')\n",
    "print(M)\n",
    "print('=')\n",
    "print(vs*M)\n",
    "\n",
    "print()\n",
    "M2 = np.matrix(\n",
    "    '0.5 0   ;'  # col 1 becomes 0.5 of col 1, and 0.5 of col 3\n",
    "    '0   0   ;'  # col 2 dropped\n",
    "    '0.5 0.5 ;'  # col 3 becomes 0.5 of col 1, and 0.5 of col 2\n",
    "    '0   0   ;'  # col 4 droppes\n",
    "    '0.5 0.5 ')  # col 5 becomes 0.5 of col 2\n",
    "\n",
    "print(vs)\n",
    "print('*')\n",
    "print(M2)\n",
    "print('=')\n",
    "print(vs*M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "invalid data type for einsum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-01698e17b09a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mM_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: invalid data type for einsum"
     ]
    }
   ],
   "source": [
    "vs = np.array([['1','2','1','2','1'], '1','2','0','2','1'])\n",
    "M = np.array(\n",
    "    [\n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']],\n",
    "        \n",
    "        [['1','0','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','1','0'],\n",
    "         ['0','0','0'],\n",
    "         ['0','0','1']]\n",
    "    ]\n",
    ")\n",
    "type(M)\n",
    "M_t = np.transpose(M,[1,2,0])\n",
    "np.matmul(M_t, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, Pair(j=32, k=64))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "Pair = collections.namedtuple('Pair', 'j, k')\n",
    "ijk_0 = (tf.constant(0), Pair(tf.constant(1), tf.constant(2)))\n",
    "c = lambda i, p: i < 10\n",
    "b = lambda i, p: (i + 1, Pair((p.j + p.k), (p.j - p.k)))\n",
    "ijk_final = tf.while_loop(c, b, ijk_0)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(ijk_final)\n",
    "#     result = session.run(tf.shape(smt))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = np.array([['1','2','1','2','1'], '1','2','0','2','1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
